{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/rishigurjar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "BASE_DIRECTORY = \"/Users/rishigurjar/Documents/mac-agent/models\"\n",
    "BNB_DIRECTORY = f\"{BASE_DIRECTORY}/bnb\"\n",
    "MLX_DIRECTORY = f\"{BASE_DIRECTORY}/mlx\"\n",
    "\n",
    "model_directory = f\"{BASE_DIRECTORY}/{MODEL_NAME}\"\n",
    "bnb_model_directory = f\"{BNB_DIRECTORY}/{MODEL_NAME}\"\n",
    "mlx_model_directory = f\"{MLX_DIRECTORY}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9edce200d31459dad337054d9885a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.save_pretrained(save_directory=model_directory)\n",
    "model.save_pretrained(save_directory=model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate, convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "tokenizer.save_pretrained(save_directory=mlx_model_directory)\n",
    "del tokenizer\n",
    "\n",
    "convert(\n",
    "    hf_path=model_directory,\n",
    "    mlx_path=mlx_model_directory,\n",
    "    quantize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(mlx_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Tell me a joke in the style of Ali-G about Trump, keep it moderately short\n",
      "and simple.\n",
      "\n",
      "Here's one I came up with:\n",
      "\n",
      "Ali-G: Yo, check it, Trump's got a new plan to make America great again, eh? He's gonna build a wall around the White House, but not just any wall, nah, it's gonna be a Trump Wall!\n",
      "\n",
      "Crowd: (laughs)\n",
      "\n",
      "Ali-G: And what's he gonna put on top of it? Trump Towers! (laughs)\n",
      "\n",
      "Ali-G: But wait, there's more! He's gonna make Mexico pay for it, right? So, how's he gonna do that? He's gonna... (pauses for effect) ...charge them for the wall! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "Ali-G: And if they don't pay up, he's gonna... (pauses) ...send Melania to collect! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "Ali-G: Yo, that's what I call a win-win situation! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "I hope you find it amusing! Let me know if you have any other requests.\n",
      "==========\n",
      "Prompt: 12.004 tokens-per-sec\n",
      "Generation: 33.227 tokens-per-sec\n",
      "and simple.\n",
      "\n",
      "Here's one I came up with:\n",
      "\n",
      "Ali-G: Yo, check it, Trump's got a new plan to make America great again, eh? He's gonna build a wall around the White House, but not just any wall, nah, it's gonna be a Trump Wall!\n",
      "\n",
      "Crowd: (laughs)\n",
      "\n",
      "Ali-G: And what's he gonna put on top of it? Trump Towers! (laughs)\n",
      "\n",
      "Ali-G: But wait, there's more! He's gonna make Mexico pay for it, right? So, how's he gonna do that? He's gonna... (pauses for effect) ...charge them for the wall! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "Ali-G: And if they don't pay up, he's gonna... (pauses) ...send Melania to collect! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "Ali-G: Yo, that's what I call a win-win situation! (laughs)\n",
      "\n",
      "Crowd: (laughs and cheers)\n",
      "\n",
      "I hope you find it amusing! Let me know if you have any other requests.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a joke in the style of Ali-G about Trump, keep it moderately short\"\n",
    "\n",
    "response = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
